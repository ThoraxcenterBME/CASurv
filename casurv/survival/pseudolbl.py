__all__ = ['PseudoLblCallback', 'PseudoLblWeightsCallback', 'PseudoLblPandasCallback', 'PseudoLblPandasAppendCallback']


from fastai.vision.all import *
from ..basics import *
from .data import *


def _modify_tensor(items, pseudo_lbls, pct=None):
    if pct is None: return pseudo_lbls
    else:
        n = len(items)
        idxs = torch.randperm(n)[:int(n*pct)]
        items[idxs] = pseudo_lbls[idxs]
        return items

class PseudoLblCallback(Callback):
    "This callback generates pseudo-labels while training"
    name = 'pseudo_lbl'
    def __init__(self, start_epoch=1, anneal=True, start=0, end=1, tls_idx=-1,
                 copy_fn=torch.clone, modify_fn=_modify_tensor):
        '''- start_epoch: Epoch in which the callback starts working.
           - anneal: Use annealing to determine the ratio of items to replace with pseudolabels.
           - start: Start value for annealing.
           - end: Final value for annealing.
           - tls_idx: Index of labels on the dataset (usually is the last: -1).
           - copy_fn: function to copy a backup of the items on the dataset (default: `torch.clone`).
           - modify_fn: function that returns the modified version of the dataset.
             `modify_fn(items, pseudo_lbls, pct)`

           Notes:
           - Default setting assumes items are of type `TensorSurvival`.'''
        store_attr()
        self.sched = SchedCos(start, end)

    def get_items(self): return self.learn.dls.train_ds.tls[self.tls_idx].items
    def set_items(self, x): self.learn.dls.train_ds.tls[self.tls_idx].items = x
    def reset_items(self): self.set_items(self.copy_fn(self.bkup))

    def get_pseudo_lbls(self, pbar=True):
        dl = self.learn.dls.train.new(shuffle=False, drop_last=False)
        m = self.learn.model.eval()
        pseudo_lbls = []
        it = progress_bar(dl) if pbar else dl
        with torch.no_grad():
            for *xb,yb in it:
                preds = m(*xb).squeeze()
                mask = (yb.cens == 1) & (preds > yb.surv)
                pseudo_lbls.append(yb.surv.where(~mask, preds).cpu())

        pseudo_lbls = torch.cat(pseudo_lbls)
        return torch.stack([pseudo_lbls, torch.zeros_like(pseudo_lbls)], dim=1)

    def set_pseudo_lbls(self):
        pseudo_lbls = self.get_pseudo_lbls(pbar=False)
        self.set_items(self.modify_fn(self.get_items(), pseudo_lbls, self.pct))

    def before_fit(self):
        self.bkup = self.copy_fn(self.get_items())
        self.pct = 0 if self.anneal else None

    def before_train(self):
        if self.epoch >= self.start_epoch: self.set_pseudo_lbls()

    def after_batch(self):
        if self.anneal and self.training:
            total = self.learn.n_epoch * self.learn.n_iter
            cur = self.learn.epoch * self.learn.n_iter + self.iter
            self.pct = self.sched(cur/total)

    def after_train(self):
        if self.epoch >= self.start_epoch: self.reset_items()


class PseudoLblWeightsCallback(Callback):
    '''This callback reduces the loss on samples generated by `PseudoLblCallback` and
    sets the `main_loss_weights` attribute on the Learner's loss function.
    '''
    name = 'pseudo_lbl_weights'
    def __init__(self, censored_ids, weight=0.5, id_idx=-1):
        '''- censored_ids: tensor with all unique patient ids of censored patients.
        - weight: weight to reduce the loss samples generated by `PseudoLblCallback`.
        - id_idx: index of the tensor with patient ids from x batches.'''
        store_attr()
    def after_pred(self):
        if self.training:
            ids = self.xb[self.id_idx]
            loss_weights = torch.ones_like(ids).float()
            mask = (ids[..., None] == self.censored_ids.to(ids.device)).any(dim=-1)
            mask = mask & (self.yb[0].cens == 0)
            loss_weights[mask] = self.weight
            self.learn.loss_func.main_loss_weights = loss_weights

    def after_train(self): self.learn.loss_func.main_loss_weights = None


def _copy_pandas(df): return df.copy()

def _modify_pandas(df, pseudo_lbls, pct, surv_col, cens_col):
    if pct is None:
        df.loc[:,surv_col] = pseudo_lbls.surv
        df.loc[:,cens_col] = pseudo_lbls.cens
    else:
        n = len(df)
        idxs = torch.randperm(n)[:int(n*pct)]
        idxs_pd = df.index[idxs]
        df.loc[idxs_pd,surv_col] = pseudo_lbls.surv[idxs].numpy()
        df.loc[idxs_pd,cens_col] = pseudo_lbls.cens[idxs].numpy()
    return df

@delegates()
class PseudoLblPandasCallback(PseudoLblCallback):
    def __init__(self, surv_col='survival', cens_col='censored', **kwargs):
        modify_fn = partial(_modify_pandas, surv_col=surv_col, cens_col=cens_col)
        super().__init__(copy_fn=_copy_pandas, modify_fn=modify_fn, **kwargs)

    def before_fit(self):
        super().before_fit()
        self.reset_items()


def _append_pandas(df, pseudo_lbls, pct, surv_col, cens_col):
    xtra = df[df.censored==1].copy()
    if pct is None:
        xtra.loc[:,surv_col] = pseudo_lbls.surv
        xtra.loc[:,cens_col] = pseudo_lbls.cens
    else:
        n = len(pseudo_lbls)
        idxs = torch.randperm(n)[:int(n*pct)]
        idxs_pd = xtra.index[idxs]
        xtra.loc[idxs_pd,surv_col] = pseudo_lbls.surv[idxs].numpy()
        xtra.loc[idxs_pd,cens_col] = pseudo_lbls.cens[idxs].numpy()
        xtra = xtra.loc[idxs_pd]

    return pd.concat([df,xtra], ignore_index=True)

@delegates()
class PseudoLblPandasAppendCallback(PseudoLblCallback):
    "Append instead of replacing values"
    def __init__(self, surv_col='survival', cens_col='censored', **kwargs):
        modify_fn = partial(_append_pandas, surv_col=surv_col, cens_col=cens_col)
        super().__init__(copy_fn=_copy_pandas, modify_fn=modify_fn, **kwargs)

    def set_items(self, x):
        self.learn.dls.train_ds.tls[self.tls_idx].items = x
        self.learn.dls.train.n = len(x)
        self.learn.dls.train.dataset.items = x

    def get_pseudo_lbls(self, pbar=True):
        dl = self.learn.dls.train.new(shuffle=False, drop_last=False)
        m = self.learn.model.eval()
        pseudo_lbls = []
        it = progress_bar(dl) if pbar else dl
        with torch.no_grad():
            for *xb,yb in it:
                mask = yb.cens == 1
                if not mask.any(): continue
                xb = [o[mask] for o in xb]
                yb = yb[mask]
                preds = m(*xb).squeeze()
                mask = (yb.cens == 1) & (preds > yb.surv)
                pseudo_lbls.append(yb.surv.where(~mask, preds).cpu())

        pseudo_lbls = torch.cat(pseudo_lbls)
        return torch.stack([pseudo_lbls, torch.zeros_like(pseudo_lbls)], dim=1)